{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompLabNGS2025 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Visit the [Gene Expression Omnibus (GEO) website](https://www.ncbi.nlm.nih.gov/geo/) and search for the accession number you were given. Review the information on the dataset and explore the publication that generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short but inclusive description of the dataset you were given. Include name of the dataset, the publication, authors, number and condition of samples and any other relevant information to allow others to understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dataset: [GSE190189](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE190189) – Induction of Immortal-like CAR-T Cells**\n",
    "\n",
    "- **Published**: March 7, 2024\n",
    "\n",
    "- **Authors**: Wang L., Jin G., Peng M.\n",
    "\n",
    "- **Organism**: Mus musculus (mouse)\n",
    "\n",
    "- **Experiment**: Bulk RNA-seq of CAR-T cells\n",
    "- **Experiment type**: Expression profiling by high throughput sequencing \n",
    "- **Samples**:\n",
    "\n",
    "    Day 10: Thy1.1⁺ CAR19TIF & sgZc3h12a CAR-T cells\n",
    "\n",
    "    3 Months: Thy1.1⁺ CAR19TIF & endogenous CD8 T cells\n",
    "\n",
    "- **Sequencer**: Illumina NovaSeq 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the entire analysis, create a conda environment with all the necessary packages and their versions to run the analysis. In the cell below, write the command you used to create the environment and the command to activate it. The name of the environment should be `<first name>_<last name>`. You can use ```bash conda list``` to list all the packages and their versions in the environment.\n",
    "\n",
    "Modify the example command below to include the packages you used in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!conda create -n Arielle_Arabov_Tomer_Oron \\\n",
    "    python=3.10 \\\n",
    "    ipykernel \\\n",
    "    sra-tools=3.2.0 \\\n",
    "    kallisto==0.50.1 \\\n",
    "    numpy=2.2.2 \\\n",
    "    pandas=2.2.3 \\\n",
    "    matplotlib=3.10.0 \\\n",
    "    scipy=1.15.1 \\\n",
    "    scikit-learn=1.6.1 \\\n",
    "    pydeseq2=0.5.0\n",
    "\n",
    "!conda activate Arielle_Arabov_Tomer_Oron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the raw fastq files\n",
    "At the bottom of the GEO page, you will find a link to the `SRA Run Selector`. Click on the link. \n",
    "\n",
    "Explain what is the SRA and SRR in gerneal and how it is used to store sequencing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence Read Archive (SRA)**\n",
    "\n",
    "A publicly accessible database maintained by the NCBI for storing raw high-throughput sequencing data.\n",
    "\n",
    "**SRA Run Accession (SRR)**\n",
    "\n",
    "A unique identifier for an individual sequencing run within an SRA study, representing a specific sample or condition.\n",
    "\n",
    "**Usage:**\n",
    "- Retrieve raw sequencing reads in FASTQ format.\n",
    "- Reanalyze data using bioinformatics pipelines.\n",
    "- Access metadata for reproducibility and interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `prefetch + fasterq-dump` command from the `SRA Toolkit` to download the fastq files. Write the command you used to download the fastq files. [SRA Tools Conda installation](https://anaconda.org/bioconda/sra-tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-02T12:10:38 prefetch.3.2.0: 1) Resolving 'SRP349257'...\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0 int: string unexpected while executing query within virtual file system module - multiple response SRR URLs for the same service 's3'\n",
      "2025-02-02T12:10:40 prefetch.3.2.0: Current preference is set to retrieve SRA Normalized Format files with full base quality scores\n",
      "2025-02-02T12:10:41 prefetch.3.2.0: 1) Downloading 'SRR17133380'...\n",
      "2025-02-02T12:10:41 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:10:41 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:11:21 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:11:24 prefetch.3.2.0:  'SRR17133380' is valid: 677896988 bytes were streamed from 677887491\n",
      "2025-02-02T12:11:24 prefetch.3.2.0: 1) 'SRR17133380' was downloaded successfully\n",
      "2025-02-02T12:11:24 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:11:24 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:11:24 prefetch.3.2.0: 1) Downloading 'SRR17133381'...\n",
      "2025-02-02T12:11:24 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:11:24 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:12:37 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:12:41 prefetch.3.2.0:  'SRR17133381' is valid: 691940522 bytes were streamed from 691936791\n",
      "2025-02-02T12:12:41 prefetch.3.2.0: 1) 'SRR17133381' was downloaded successfully\n",
      "2025-02-02T12:12:41 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:12:41 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:12:41 prefetch.3.2.0: 1) Downloading 'SRR17133382'...\n",
      "2025-02-02T12:12:41 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:12:41 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:14:19 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:14:23 prefetch.3.2.0:  'SRR17133382' is valid: 691207985 bytes were streamed from 691192343\n",
      "2025-02-02T12:14:23 prefetch.3.2.0: 1) 'SRR17133382' was downloaded successfully\n",
      "2025-02-02T12:14:23 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:14:23 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:14:23 prefetch.3.2.0: 1) Downloading 'SRR17133383'...\n",
      "2025-02-02T12:14:23 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:14:23 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:14:53 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:14:55 prefetch.3.2.0:  'SRR17133383' is valid: 511169456 bytes were streamed from 511166967\n",
      "2025-02-02T12:14:55 prefetch.3.2.0: 1) 'SRR17133383' was downloaded successfully\n",
      "2025-02-02T12:14:55 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:14:55 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:14:55 prefetch.3.2.0: 1) Downloading 'SRR17133384'...\n",
      "2025-02-02T12:14:55 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:14:55 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:15:34 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:15:37 prefetch.3.2.0:  'SRR17133384' is valid: 624829770 bytes were streamed from 624820727\n",
      "2025-02-02T12:15:37 prefetch.3.2.0: 1) 'SRR17133384' was downloaded successfully\n",
      "2025-02-02T12:15:37 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:15:37 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:15:37 prefetch.3.2.0: 1) Downloading 'SRR17133385'...\n",
      "2025-02-02T12:15:37 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:15:37 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:17:12 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:17:15 prefetch.3.2.0:  'SRR17133385' is valid: 687361740 bytes were streamed from 687356157\n",
      "2025-02-02T12:17:15 prefetch.3.2.0: 1) 'SRR17133385' was downloaded successfully\n",
      "2025-02-02T12:17:15 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:17:16 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:17:16 prefetch.3.2.0: 1) Downloading 'SRR17133386'...\n",
      "2025-02-02T12:17:16 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:17:16 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:18:28 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:18:31 prefetch.3.2.0:  'SRR17133386' is valid: 702442670 bytes were streamed from 702428695\n",
      "2025-02-02T12:18:31 prefetch.3.2.0: 1) 'SRR17133386' was downloaded successfully\n",
      "2025-02-02T12:18:31 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:18:31 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "2025-02-02T12:18:31 prefetch.3.2.0: 1) Downloading 'SRR17133387'...\n",
      "2025-02-02T12:18:31 prefetch.3.2.0:  SRA Normalized Format file is being retrieved\n",
      "2025-02-02T12:18:31 prefetch.3.2.0:  Downloading via HTTPS...\n",
      "2025-02-02T12:19:26 prefetch.3.2.0:  HTTPS download succeed\n",
      "2025-02-02T12:19:29 prefetch.3.2.0:  'SRR17133387' is valid: 662897842 bytes were streamed from 662889893\n",
      "2025-02-02T12:19:29 prefetch.3.2.0: 1) 'SRR17133387' was downloaded successfully\n",
      "2025-02-02T12:19:29 prefetch.3.2.0: 1) Resolving 'SRP349257's dependencies...\n",
      "2025-02-02T12:19:29 prefetch.3.2.0: 'SRP349257' has 0 unresolved dependencies\n",
      "./SRP349257/SRR17133380\n",
      "spots read      : 23,372,262\n",
      "reads read      : 46,744,524\n",
      "reads written   : 23,372,262\n",
      "reads 0-length  : 23,372,262\n",
      "./SRP349257/SRR17133381\n",
      "spots read      : 23,711,155\n",
      "reads read      : 47,422,310\n",
      "reads written   : 23,711,155\n",
      "reads 0-length  : 23,711,155\n",
      "./SRP349257/SRR17133382\n",
      "spots read      : 23,834,934\n",
      "reads read      : 47,669,868\n",
      "reads written   : 23,834,934\n",
      "reads 0-length  : 23,834,934\n",
      "./SRP349257/SRR17133383\n",
      "spots read      : 18,278,165\n",
      "reads read      : 36,556,330\n",
      "reads written   : 18,278,165\n",
      "reads 0-length  : 18,278,165\n",
      "./SRP349257/SRR17133384\n",
      "spots read      : 21,376,164\n",
      "reads read      : 42,752,328\n",
      "reads written   : 21,376,164\n",
      "reads 0-length  : 21,376,164\n",
      "./SRP349257/SRR17133385\n",
      "spots read      : 23,837,608\n",
      "reads read      : 47,675,216\n",
      "reads written   : 23,837,608\n",
      "reads 0-length  : 23,837,608\n",
      "./SRP349257/SRR17133386\n",
      "spots read      : 23,670,852\n",
      "reads read      : 47,341,704\n",
      "reads written   : 23,670,852\n",
      "reads 0-length  : 23,670,852\n",
      "./SRP349257/SRR17133387\n",
      "spots read      : 23,255,124\n",
      "reads read      : 46,510,248\n",
      "reads written   : 23,255,124\n",
      "reads 0-length  : 23,255,124\n"
     ]
    }
   ],
   "source": [
    "# extracting SRR as folders\n",
    "!prefetch SRP349257 -O SRP349257\n",
    "# converting into fastqc\n",
    "!for sra_folder in ./SRP349257/*; do echo \"$sra_folder\"; fasterq-dump \"$sra_folder\" -e 4 --outdir SRP349257_fastq; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control\n",
    "Below include all the commands, figures and output you used and generated to perform quality control on the fastq files. You can use `fastqc`, `trimmomatic`, `multiqc` and any other tool you find necessary to give a comprehensive QC and filtering of the data.\n",
    "\n",
    "Write a brief description of the quality control process and explain the figures and tables you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment and DGE\n",
    "\n",
    "Similar to what we did in class and in the assignemnts, you will now run through the steps of mapping and conducting DGE. Include below an elaborate report of your analysis, include study desgin, example count and normalized count matrices, draw summary satistics and every other information you think is relevent to report your findings (if any)\n",
    "\n",
    "Running `STAR` on a laptop or PC is almost impossible we will use one of the more recent pseudo alighners, ['kallisto`](https://pachterlab.github.io/kallisto/manual), which will allow us to quantify transcript level information in a more efficent manner.\n",
    "\n",
    "1. Download the index files from kallisto's GitHub\n",
    "```bash\n",
    "wget https://github.com/pachterlab/kallisto-transcriptome-indices/releases/download/v1/mouse_index_standard.tar.xz\n",
    "tar -xf mouse_index_standard.tar.xz\n",
    "```\n",
    "2. Install and run `kallisto`\n",
    "```bash\n",
    "conda install -c bioconda -c conda-forge kallisto==0.50.1\n",
    "kallisto quant -i index.idx -o output -t 16 sample.fastq \n",
    "```\n",
    "\n",
    "In the `output` directory you should have `abundance.tsv`. View the file and make sure you understand it. Produce quantification to all your samples.\n",
    "We now need to sum up transcript level counts to gene level counts before proceeding to `pyDESEQ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load transcript to gene mapping file\n",
    "t2g_file = 't2g.txt'\n",
    "t2g_df = pd.read_csv(t2g_file, sep='\\t', usecols=[0, 2], header=None, names=['transcript_id', 'gene_id'])\n",
    "\n",
    "abundance_file = 'output/abundance.tsv'\n",
    "abundance_df = pd.read_csv(abundance_file, sep='\\t')\n",
    "\n",
    "# Merge transcript abundance with gene mapping\n",
    "merged_df = pd.merge(abundance_df, t2g_df, left_on='target_id', right_on='transcript_id', how='inner')\n",
    "\n",
    "# Sum abundances at the gene level\n",
    "gene_abundance_df = merged_df.groupby('gene_id')['tpm'].sum().reset_index()\n",
    "\n",
    "# Save gene-level abundance to a new file\n",
    "print(gene_abundance_df)\n",
    "gene_abundance_df.to_csv('gene_abundance.csv', index=False)\n",
    "```\n",
    "\n",
    "Do you understand the code? Include `print` statments and rerun until you understnad what's going on fully\n",
    "\n",
    "Do this operation to each of your sample's abundance file. Using python or a different tool merge all samples to a single file.\n",
    "\n",
    "You should now be able to take the resulting `all_samples_gene_abundance.csv` and use it in `pyDeSeq2` as we did in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene Set Enrichment Analysis\n",
    "\n",
    "Explain what GSEA is and what insights can we draw from it.\n",
    "\n",
    "Execute a GSEA analysis using [pyGSEA](https://gseapy.readthedocs.io/en/latest/introduction.html) and summarize your results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This excerise is meant to 'through' in the water and might prove challanging. Attempt to work in groups to overcome challanges. \n",
    "\n",
    "If you get stuck try to post an issue, but give as much background and explanation of your problem as possible so I could help\n",
    "\n",
    "### Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Arielle_Arabov_Tomer_Oron]",
   "language": "python",
   "name": "conda-env-Arielle_Arabov_Tomer_Oron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
